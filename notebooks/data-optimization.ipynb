{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Optimization: CSV to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook converts the large `master_dataframe.csv` file into the Parquet format using a robust, chunk-by-chunk method that directly uses the `pyarrow` library to avoid potential versioning conflicts with pandas' `append` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "csv_path = '/Users/mariostam/Project files/master_dataframe.csv'\n",
    "parquet_path = '/Users/mariostam/Project files/master_dataframe.parquet'\n",
    "chunk_size = 100000  # 100,000 rows per chunk\n",
    "\n",
    "# Create a reader object that yields chunks\n",
    "reader = pd.read_csv(csv_path, chunksize=chunk_size)\n",
    "\n",
    "# Get the first chunk to infer the schema\n",
    "print(\"Inferring schema from the first chunk...\")\n",
    "first_chunk = next(reader)\n",
    "schema = pa.Table.from_pandas(first_chunk).schema\n",
    "\n",
    "# Set up the Parquet writer with the inferred schema\n",
    "writer = pq.ParquetWriter(parquet_path, schema)\n",
    "\n",
    "# Write the first chunk\n",
    "table = pa.Table.from_pandas(first_chunk, schema=schema)\n",
    "writer.write_table(table)\n",
    "\n",
    "# Get total lines for a more accurate progress bar\n",
    "total_lines = sum(1 for line in open(csv_path, 'r')) - 1 # Subtract header\n",
    "\n",
    "print(f\"Converting {total_lines:,} rows from CSV to Parquet...\")\n",
    "\n",
    "# Loop through the rest of the chunks and write them\n",
    "# We already processed one chunk, so we subtract it from the total for tqdm\n",
    "for chunk in tqdm(reader, total=(total_lines // chunk_size) - 1):\n",
    "    table = pa.Table.from_pandas(chunk, schema=schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "# Close the writer to finalize the file\n",
    "writer.close()\n",
    "\n",
    "print(\"Conversion complete!\")\n",
    "print(f\"Parquet file saved to: {parquet_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
